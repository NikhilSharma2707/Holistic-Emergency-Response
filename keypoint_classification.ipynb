{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T10:43:33.908507500Z",
     "start_time": "2024-08-31T10:43:33.617360Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 各パス指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-31T10:43:33.647878800Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = 'model/keypoint_classifier/keypoint.csv'\n",
    "model_save_path = \"model/keypoint_classifier/keypoint_classifier.keras\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分類数設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-31T10:43:33.655878700Z"
    }
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T10:43:34.670740400Z",
     "start_time": "2024-08-31T10:43:33.666878800Z"
    }
   },
   "outputs": [],
   "source": [
    "X_dataset = np.loadtxt(dataset, delimiter=',', dtype='float32', usecols=list(range(1, (21 * 2) + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dataset = np.loadtxt(dataset, delimiter=',', dtype='int32', usecols=(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_dataset, y_dataset, train_size=0.75, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input((21 * 2, )),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(20, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax'),\n",
    "    tf.keras.layers.Dense(7, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1mModel: \"sequential\"\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">860</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">210</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                        \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape               \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m        Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dropout (\u001B[38;5;33mDropout\u001B[0m)                    │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m42\u001B[0m)                  │               \u001B[38;5;34m0\u001B[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001B[38;5;33mDense\u001B[0m)                        │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m20\u001B[0m)                  │             \u001B[38;5;34m860\u001B[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001B[38;5;33mDropout\u001B[0m)                  │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m20\u001B[0m)                  │               \u001B[38;5;34m0\u001B[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001B[38;5;33mDense\u001B[0m)                      │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m10\u001B[0m)                  │             \u001B[38;5;34m210\u001B[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (\u001B[38;5;33mDense\u001B[0m)                      │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m6\u001B[0m)                   │              \u001B[38;5;34m66\u001B[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (\u001B[38;5;33mDense\u001B[0m)                      │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m7\u001B[0m)                   │              \u001B[38;5;34m49\u001B[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,185</span> (4.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m1,185\u001B[0m (4.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,185</span> (4.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m1,185\u001B[0m (4.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()  # tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルチェックポイントのコールバック\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    model_save_path, verbose=1, save_weights_only=False)\n",
    "# 早期打ち切り用コールバック\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(patience=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルコンパイル\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001B[1m23/35\u001B[0m \u001B[32m━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.0254 - loss: 2.0442\n",
      "Epoch 1: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 10ms/step - accuracy: 0.0274 - loss: 2.0357 - val_accuracy: 0.0779 - val_loss: 1.9667\n",
      "Epoch 2/1000\n",
      "\u001B[1m30/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.1055 - loss: 1.9492 \n",
      "Epoch 2: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.1140 - loss: 1.9458 - val_accuracy: 0.2593 - val_loss: 1.8774\n",
      "Epoch 3/1000\n",
      "\u001B[1m32/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.2732 - loss: 1.8656 \n",
      "Epoch 3: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.2736 - loss: 1.8637 - val_accuracy: 0.2701 - val_loss: 1.7878\n",
      "Epoch 4/1000\n",
      "\u001B[1m23/35\u001B[0m \u001B[32m━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.3117 - loss: 1.7843 \n",
      "Epoch 4: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.3056 - loss: 1.7799 - val_accuracy: 0.2965 - val_loss: 1.7103\n",
      "Epoch 5/1000\n",
      "\u001B[1m31/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.2730 - loss: 1.7387\n",
      "Epoch 5: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.2739 - loss: 1.7361 - val_accuracy: 0.2817 - val_loss: 1.6611\n",
      "Epoch 6/1000\n",
      "\u001B[1m29/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.2604 - loss: 1.6903 \n",
      "Epoch 6: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.2636 - loss: 1.6875 - val_accuracy: 0.2823 - val_loss: 1.6213\n",
      "Epoch 7/1000\n",
      "\u001B[1m30/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.2861 - loss: 1.6485 \n",
      "Epoch 7: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.2865 - loss: 1.6472 - val_accuracy: 0.3114 - val_loss: 1.5878\n",
      "Epoch 8/1000\n",
      "\u001B[1m30/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.2885 - loss: 1.6217\n",
      "Epoch 8: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.2890 - loss: 1.6201 - val_accuracy: 0.3108 - val_loss: 1.5593\n",
      "Epoch 9/1000\n",
      "\u001B[1m34/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.3039 - loss: 1.5956\n",
      "Epoch 9: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.3034 - loss: 1.5951 - val_accuracy: 0.3338 - val_loss: 1.5308\n",
      "Epoch 10/1000\n",
      "\u001B[1m18/35\u001B[0m \u001B[32m━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.3061 - loss: 1.5748 \n",
      "Epoch 10: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.3065 - loss: 1.5702 - val_accuracy: 0.3602 - val_loss: 1.5049\n",
      "Epoch 11/1000\n",
      "\u001B[1m25/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.3146 - loss: 1.5496 \n",
      "Epoch 11: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.3189 - loss: 1.5464 - val_accuracy: 0.4049 - val_loss: 1.4801\n",
      "Epoch 12/1000\n",
      "\u001B[1m12/35\u001B[0m \u001B[32m━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.3563 - loss: 1.5153 \n",
      "Epoch 12: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.3530 - loss: 1.5158 - val_accuracy: 0.3805 - val_loss: 1.4589\n",
      "Epoch 13/1000\n",
      "\u001B[1m25/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.3673 - loss: 1.4955 \n",
      "Epoch 13: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.3656 - loss: 1.4946 - val_accuracy: 0.3764 - val_loss: 1.4411\n",
      "Epoch 14/1000\n",
      "\u001B[1m25/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.3577 - loss: 1.4798  \n",
      "Epoch 14: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.3578 - loss: 1.4797 - val_accuracy: 0.3832 - val_loss: 1.4237\n",
      "Epoch 15/1000\n",
      "\u001B[1m23/35\u001B[0m \u001B[32m━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.3776 - loss: 1.4508 \n",
      "Epoch 15: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.3730 - loss: 1.4532 - val_accuracy: 0.3717 - val_loss: 1.4130\n",
      "Epoch 16/1000\n",
      "\u001B[1m29/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.3484 - loss: 1.4571\n",
      "Epoch 16: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.3490 - loss: 1.4572 - val_accuracy: 0.3744 - val_loss: 1.4027\n",
      "Epoch 17/1000\n",
      "\u001B[1m31/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.3612 - loss: 1.4353\n",
      "Epoch 17: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.3615 - loss: 1.4355 - val_accuracy: 0.3825 - val_loss: 1.3881\n",
      "Epoch 18/1000\n",
      "\u001B[1m16/35\u001B[0m \u001B[32m━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.3801 - loss: 1.4237 \n",
      "Epoch 18: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.3744 - loss: 1.4230 - val_accuracy: 0.3825 - val_loss: 1.3763\n",
      "Epoch 19/1000\n",
      "\u001B[1m15/35\u001B[0m \u001B[32m━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.3870 - loss: 1.4136 \n",
      "Epoch 19: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.3763 - loss: 1.4222 - val_accuracy: 0.3859 - val_loss: 1.3657\n",
      "Epoch 20/1000\n",
      "\u001B[1m18/35\u001B[0m \u001B[32m━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.3747 - loss: 1.4082 \n",
      "Epoch 20: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.3708 - loss: 1.4087 - val_accuracy: 0.3839 - val_loss: 1.3602\n",
      "Epoch 21/1000\n",
      "\u001B[1m21/35\u001B[0m \u001B[32m━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.3609 - loss: 1.4068 \n",
      "Epoch 21: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.3627 - loss: 1.4046 - val_accuracy: 0.3832 - val_loss: 1.3521\n",
      "Epoch 22/1000\n",
      "\u001B[1m22/35\u001B[0m \u001B[32m━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.3658 - loss: 1.3912 \n",
      "Epoch 22: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.3675 - loss: 1.3914 - val_accuracy: 0.3866 - val_loss: 1.3435\n",
      "Epoch 23/1000\n",
      "\u001B[1m30/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.3617 - loss: 1.3759\n",
      "Epoch 23: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - accuracy: 0.3627 - loss: 1.3763 - val_accuracy: 0.3873 - val_loss: 1.3369\n",
      "Epoch 24/1000\n",
      "\u001B[1m28/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.3710 - loss: 1.3739\n",
      "Epoch 24: saving model to model/keypoint_classifier/keypoint_classifier.keras\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.3707 - loss: 1.3740 - val_accuracy: 0.3879 - val_loss: 1.3297\n",
      "Epoch 25/1000\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.3609 - loss: 1.3877\n",
      "Epoch 25: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.3610 - loss: 1.3873 - val_accuracy: 0.3920 - val_loss: 1.3250\n",
      "Epoch 26/1000\n",
      "\u001B[1m32/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.3737 - loss: 1.3605\n",
      "Epoch 26: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.3744 - loss: 1.3596 - val_accuracy: 0.3859 - val_loss: 1.3198\n",
      "Epoch 27/1000\n",
      "\u001B[1m19/35\u001B[0m \u001B[32m━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.3538 - loss: 1.3740 \n",
      "Epoch 27: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.3620 - loss: 1.3681 - val_accuracy: 0.3900 - val_loss: 1.3149\n",
      "Epoch 28/1000\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.3856 - loss: 1.3481\n",
      "Epoch 28: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.3854 - loss: 1.3481 - val_accuracy: 0.3893 - val_loss: 1.3082\n",
      "Epoch 29/1000\n",
      "\u001B[1m24/35\u001B[0m \u001B[32m━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.3887 - loss: 1.3487\n",
      "Epoch 29: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.3841 - loss: 1.3478 - val_accuracy: 0.3940 - val_loss: 1.3010\n",
      "Epoch 30/1000\n",
      "\u001B[1m22/35\u001B[0m \u001B[32m━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.3986 - loss: 1.3354 \n",
      "Epoch 30: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.3919 - loss: 1.3377 - val_accuracy: 0.3954 - val_loss: 1.2978\n",
      "Epoch 31/1000\n",
      "\u001B[1m26/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.3834 - loss: 1.3379 \n",
      "Epoch 31: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.3815 - loss: 1.3398 - val_accuracy: 0.3947 - val_loss: 1.2934\n",
      "Epoch 32/1000\n",
      "\u001B[1m29/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.3785 - loss: 1.3410 \n",
      "Epoch 32: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.3783 - loss: 1.3393 - val_accuracy: 0.3968 - val_loss: 1.2889\n",
      "Epoch 33/1000\n",
      "\u001B[1m30/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.3930 - loss: 1.3322 \n",
      "Epoch 33: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.3908 - loss: 1.3326 - val_accuracy: 0.3961 - val_loss: 1.2864\n",
      "Epoch 34/1000\n",
      "\u001B[1m23/35\u001B[0m \u001B[32m━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.3706 - loss: 1.3428 \n",
      "Epoch 34: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.3742 - loss: 1.3397 - val_accuracy: 0.3961 - val_loss: 1.2831\n",
      "Epoch 35/1000\n",
      "\u001B[1m32/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.3896 - loss: 1.3186\n",
      "Epoch 35: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.3890 - loss: 1.3185 - val_accuracy: 0.3927 - val_loss: 1.2766\n",
      "Epoch 36/1000\n",
      "\u001B[1m34/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.3912 - loss: 1.3167\n",
      "Epoch 36: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.3914 - loss: 1.3166 - val_accuracy: 0.3947 - val_loss: 1.2720\n",
      "Epoch 37/1000\n",
      "\u001B[1m19/35\u001B[0m \u001B[32m━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.3777 - loss: 1.3014 \n",
      "Epoch 37: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.3832 - loss: 1.3055 - val_accuracy: 0.4022 - val_loss: 1.2662\n",
      "Epoch 38/1000\n",
      "\u001B[1m28/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.3983 - loss: 1.3208 \n",
      "Epoch 38: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.3977 - loss: 1.3183 - val_accuracy: 0.4177 - val_loss: 1.2568\n",
      "Epoch 39/1000\n",
      "\u001B[1m29/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.4002 - loss: 1.3129 \n",
      "Epoch 39: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.4024 - loss: 1.3105 - val_accuracy: 0.4279 - val_loss: 1.2448\n",
      "Epoch 40/1000\n",
      "\u001B[1m25/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.4133 - loss: 1.2878 \n",
      "Epoch 40: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.4165 - loss: 1.2890 - val_accuracy: 0.4360 - val_loss: 1.2326\n",
      "Epoch 41/1000\n",
      "\u001B[1m25/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.4223 - loss: 1.2785 \n",
      "Epoch 41: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.4203 - loss: 1.2806 - val_accuracy: 0.4441 - val_loss: 1.2224\n",
      "Epoch 42/1000\n",
      "\u001B[1m16/35\u001B[0m \u001B[32m━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.4588 - loss: 1.2674 \n",
      "Epoch 42: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.4501 - loss: 1.2725 - val_accuracy: 0.4428 - val_loss: 1.2111\n",
      "Epoch 43/1000\n",
      "\u001B[1m26/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.4306 - loss: 1.2805 \n",
      "Epoch 43: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.4337 - loss: 1.2773 - val_accuracy: 0.4543 - val_loss: 1.1985\n",
      "Epoch 44/1000\n",
      "\u001B[1m20/35\u001B[0m \u001B[32m━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.4362 - loss: 1.2705 \n",
      "Epoch 44: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.4363 - loss: 1.2669 - val_accuracy: 0.4712 - val_loss: 1.1865\n",
      "Epoch 45/1000\n",
      "\u001B[1m22/35\u001B[0m \u001B[32m━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.4338 - loss: 1.2446 \n",
      "Epoch 45: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.4375 - loss: 1.2452 - val_accuracy: 0.4753 - val_loss: 1.1736\n",
      "Epoch 46/1000\n",
      "\u001B[1m21/35\u001B[0m \u001B[32m━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.4582 - loss: 1.2315 \n",
      "Epoch 46: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.4570 - loss: 1.2385 - val_accuracy: 0.5267 - val_loss: 1.1609\n",
      "Epoch 47/1000\n",
      "\u001B[1m21/35\u001B[0m \u001B[32m━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.5214 - loss: 1.2143 \n",
      "Epoch 47: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.5157 - loss: 1.2163 - val_accuracy: 0.5640 - val_loss: 1.1437\n",
      "Epoch 48/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m22/35\u001B[0m \u001B[32m━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.5100 - loss: 1.2325 \n",
      "Epoch 48: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.5123 - loss: 1.2291 - val_accuracy: 0.5999 - val_loss: 1.1327\n",
      "Epoch 49/1000\n",
      "\u001B[1m29/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.5211 - loss: 1.2045 \n",
      "Epoch 49: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.5206 - loss: 1.2052 - val_accuracy: 0.5978 - val_loss: 1.1184\n",
      "Epoch 50/1000\n",
      "\u001B[1m18/35\u001B[0m \u001B[32m━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.5285 - loss: 1.1856 \n",
      "Epoch 50: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.5278 - loss: 1.1886 - val_accuracy: 0.6005 - val_loss: 1.1057\n",
      "Epoch 51/1000\n",
      "\u001B[1m29/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.5152 - loss: 1.2058 \n",
      "Epoch 51: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.5182 - loss: 1.2026 - val_accuracy: 0.6242 - val_loss: 1.0888\n",
      "Epoch 52/1000\n",
      "\u001B[1m28/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.5423 - loss: 1.1766 \n",
      "Epoch 52: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.5413 - loss: 1.1766 - val_accuracy: 0.6391 - val_loss: 1.0705\n",
      "Epoch 53/1000\n",
      "\u001B[1m27/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.5582 - loss: 1.1482 \n",
      "Epoch 53: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.5538 - loss: 1.1521 - val_accuracy: 0.6283 - val_loss: 1.0636\n",
      "Epoch 54/1000\n",
      "\u001B[1m26/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.5250 - loss: 1.1635 \n",
      "Epoch 54: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.5281 - loss: 1.1603 - val_accuracy: 0.6290 - val_loss: 1.0515\n",
      "Epoch 55/1000\n",
      "\u001B[1m23/35\u001B[0m \u001B[32m━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.5236 - loss: 1.1611 \n",
      "Epoch 55: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.5273 - loss: 1.1573 - val_accuracy: 0.6398 - val_loss: 1.0531\n",
      "Epoch 56/1000\n",
      "\u001B[1m27/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.5409 - loss: 1.1549 \n",
      "Epoch 56: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.5445 - loss: 1.1528 - val_accuracy: 0.6209 - val_loss: 1.0334\n",
      "Epoch 57/1000\n",
      "\u001B[1m27/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.5658 - loss: 1.1159 \n",
      "Epoch 57: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.5647 - loss: 1.1182 - val_accuracy: 0.6215 - val_loss: 1.0263\n",
      "Epoch 58/1000\n",
      "\u001B[1m28/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.5764 - loss: 1.1123 \n",
      "Epoch 58: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.5732 - loss: 1.1130 - val_accuracy: 0.6412 - val_loss: 1.0132\n",
      "Epoch 59/1000\n",
      "\u001B[1m25/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.5695 - loss: 1.1033 \n",
      "Epoch 59: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.5658 - loss: 1.1053 - val_accuracy: 0.6398 - val_loss: 1.0054\n",
      "Epoch 60/1000\n",
      "\u001B[1m27/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.5706 - loss: 1.0993 \n",
      "Epoch 60: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.5690 - loss: 1.1002 - val_accuracy: 0.6737 - val_loss: 0.9887\n",
      "Epoch 61/1000\n",
      "\u001B[1m23/35\u001B[0m \u001B[32m━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.5871 - loss: 1.0757 \n",
      "Epoch 61: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.5772 - loss: 1.0839 - val_accuracy: 0.6324 - val_loss: 0.9825\n",
      "Epoch 62/1000\n",
      "\u001B[1m21/35\u001B[0m \u001B[32m━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.5718 - loss: 1.0901 \n",
      "Epoch 62: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.5701 - loss: 1.0933 - val_accuracy: 0.6527 - val_loss: 0.9702\n",
      "Epoch 63/1000\n",
      "\u001B[1m21/35\u001B[0m \u001B[32m━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.5582 - loss: 1.1089 \n",
      "Epoch 63: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.5633 - loss: 1.1001 - val_accuracy: 0.6459 - val_loss: 0.9644\n",
      "Epoch 64/1000\n",
      "\u001B[1m27/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.5732 - loss: 1.0862 \n",
      "Epoch 64: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.5713 - loss: 1.0874 - val_accuracy: 0.6378 - val_loss: 0.9646\n",
      "Epoch 65/1000\n",
      "\u001B[1m28/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6076 - loss: 1.0468 \n",
      "Epoch 65: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.6025 - loss: 1.0508 - val_accuracy: 0.6398 - val_loss: 0.9573\n",
      "Epoch 66/1000\n",
      "\u001B[1m23/35\u001B[0m \u001B[32m━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.5588 - loss: 1.0765 \n",
      "Epoch 66: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.5619 - loss: 1.0761 - val_accuracy: 0.6770 - val_loss: 0.9342\n",
      "Epoch 67/1000\n",
      "\u001B[1m22/35\u001B[0m \u001B[32m━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.5731 - loss: 1.0896 \n",
      "Epoch 67: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.5738 - loss: 1.0837 - val_accuracy: 0.6452 - val_loss: 0.9433\n",
      "Epoch 68/1000\n",
      "\u001B[1m26/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.5787 - loss: 1.0550\n",
      "Epoch 68: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.5793 - loss: 1.0547 - val_accuracy: 0.6445 - val_loss: 0.9405\n",
      "Epoch 69/1000\n",
      "\u001B[1m30/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.5837 - loss: 1.0585\n",
      "Epoch 69: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.5823 - loss: 1.0587 - val_accuracy: 0.6608 - val_loss: 0.9204\n",
      "Epoch 70/1000\n",
      "\u001B[1m32/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.5725 - loss: 1.0605\n",
      "Epoch 70: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.5730 - loss: 1.0596 - val_accuracy: 0.6479 - val_loss: 0.9195\n",
      "Epoch 71/1000\n",
      "\u001B[1m23/35\u001B[0m \u001B[32m━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.5785 - loss: 1.0534 \n",
      "Epoch 71: saving model to model/keypoint_classifier/keypoint_classifier.keras\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.5781 - loss: 1.0504 - val_accuracy: 0.6236 - val_loss: 0.9311\n",
      "Epoch 72/1000\n",
      "\u001B[1m20/35\u001B[0m \u001B[32m━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.5837 - loss: 1.0381 \n",
      "Epoch 72: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.5838 - loss: 1.0415 - val_accuracy: 0.6249 - val_loss: 0.9161\n",
      "Epoch 73/1000\n",
      "\u001B[1m17/35\u001B[0m \u001B[32m━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.5959 - loss: 1.0255 \n",
      "Epoch 73: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.5947 - loss: 1.0284 - val_accuracy: 0.6229 - val_loss: 0.9094\n",
      "Epoch 74/1000\n",
      "\u001B[1m25/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.5825 - loss: 1.0443 \n",
      "Epoch 74: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.5827 - loss: 1.0441 - val_accuracy: 0.6391 - val_loss: 0.8985\n",
      "Epoch 75/1000\n",
      "\u001B[1m14/35\u001B[0m \u001B[32m━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.5919 - loss: 1.0600 \n",
      "Epoch 75: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.5926 - loss: 1.0470 - val_accuracy: 0.6242 - val_loss: 0.9152\n",
      "Epoch 76/1000\n",
      "\u001B[1m24/35\u001B[0m \u001B[32m━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.5983 - loss: 1.0189 \n",
      "Epoch 76: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.5972 - loss: 1.0204 - val_accuracy: 0.6242 - val_loss: 0.9048\n",
      "Epoch 77/1000\n",
      "\u001B[1m23/35\u001B[0m \u001B[32m━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.5871 - loss: 1.0469 \n",
      "Epoch 77: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.5898 - loss: 1.0407 - val_accuracy: 0.6236 - val_loss: 0.8987\n",
      "Epoch 78/1000\n",
      "\u001B[1m26/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6004 - loss: 1.0328 \n",
      "Epoch 78: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.5984 - loss: 1.0305 - val_accuracy: 0.6202 - val_loss: 0.9039\n",
      "Epoch 79/1000\n",
      "\u001B[1m27/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.5976 - loss: 1.0120 \n",
      "Epoch 79: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.5961 - loss: 1.0168 - val_accuracy: 0.6093 - val_loss: 0.9071\n",
      "Epoch 80/1000\n",
      "\u001B[1m27/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.5802 - loss: 1.0339 \n",
      "Epoch 80: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.5824 - loss: 1.0324 - val_accuracy: 0.6276 - val_loss: 0.8971\n",
      "Epoch 81/1000\n",
      "\u001B[1m34/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6010 - loss: 1.0045\n",
      "Epoch 81: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.6010 - loss: 1.0048 - val_accuracy: 0.6297 - val_loss: 0.8884\n",
      "Epoch 82/1000\n",
      "\u001B[1m27/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6070 - loss: 1.0175 \n",
      "Epoch 82: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6059 - loss: 1.0161 - val_accuracy: 0.6263 - val_loss: 0.8920\n",
      "Epoch 83/1000\n",
      "\u001B[1m24/35\u001B[0m \u001B[32m━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6089 - loss: 0.9984 \n",
      "Epoch 83: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6083 - loss: 0.9997 - val_accuracy: 0.6229 - val_loss: 0.8825\n",
      "Epoch 84/1000\n",
      "\u001B[1m24/35\u001B[0m \u001B[32m━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6098 - loss: 0.9948 \n",
      "Epoch 84: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6070 - loss: 0.9963 - val_accuracy: 0.6175 - val_loss: 0.8911\n",
      "Epoch 85/1000\n",
      "\u001B[1m25/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6105 - loss: 0.9915 \n",
      "Epoch 85: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6111 - loss: 0.9955 - val_accuracy: 0.6209 - val_loss: 0.8742\n",
      "Epoch 86/1000\n",
      "\u001B[1m29/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6046 - loss: 0.9894 \n",
      "Epoch 86: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6047 - loss: 0.9907 - val_accuracy: 0.6107 - val_loss: 0.8847\n",
      "Epoch 87/1000\n",
      "\u001B[1m25/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6132 - loss: 0.9904 \n",
      "Epoch 87: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6099 - loss: 0.9968 - val_accuracy: 0.6168 - val_loss: 0.8840\n",
      "Epoch 88/1000\n",
      "\u001B[1m26/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6042 - loss: 1.0065 \n",
      "Epoch 88: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6047 - loss: 1.0028 - val_accuracy: 0.6242 - val_loss: 0.8732\n",
      "Epoch 89/1000\n",
      "\u001B[1m27/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6207 - loss: 0.9809 \n",
      "Epoch 89: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6211 - loss: 0.9831 - val_accuracy: 0.6188 - val_loss: 0.8756\n",
      "Epoch 90/1000\n",
      "\u001B[1m26/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6114 - loss: 0.9916 \n",
      "Epoch 90: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6115 - loss: 0.9906 - val_accuracy: 0.6229 - val_loss: 0.8727\n",
      "Epoch 91/1000\n",
      "\u001B[1m21/35\u001B[0m \u001B[32m━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6032 - loss: 1.0091 \n",
      "Epoch 91: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6036 - loss: 1.0032 - val_accuracy: 0.6154 - val_loss: 0.8740\n",
      "Epoch 92/1000\n",
      "\u001B[1m30/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6077 - loss: 0.9906 \n",
      "Epoch 92: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.6094 - loss: 0.9897 - val_accuracy: 0.5931 - val_loss: 0.8937\n",
      "Epoch 93/1000\n",
      "\u001B[1m23/35\u001B[0m \u001B[32m━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6239 - loss: 0.9810 \n",
      "Epoch 93: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6234 - loss: 0.9828 - val_accuracy: 0.6236 - val_loss: 0.8575\n",
      "Epoch 94/1000\n",
      "\u001B[1m27/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6145 - loss: 0.9933 \n",
      "Epoch 94: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6136 - loss: 0.9932 - val_accuracy: 0.6168 - val_loss: 0.8560\n",
      "Epoch 95/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m18/35\u001B[0m \u001B[32m━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6236 - loss: 0.9746 \n",
      "Epoch 95: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6181 - loss: 0.9743 - val_accuracy: 0.6188 - val_loss: 0.8560\n",
      "Epoch 96/1000\n",
      "\u001B[1m26/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6093 - loss: 0.9854 \n",
      "Epoch 96: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6104 - loss: 0.9865 - val_accuracy: 0.6100 - val_loss: 0.8564\n",
      "Epoch 97/1000\n",
      "\u001B[1m25/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6346 - loss: 0.9674 \n",
      "Epoch 97: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6309 - loss: 0.9709 - val_accuracy: 0.6242 - val_loss: 0.8438\n",
      "Epoch 98/1000\n",
      "\u001B[1m31/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.5999 - loss: 0.9954 \n",
      "Epoch 98: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.6014 - loss: 0.9944 - val_accuracy: 0.6215 - val_loss: 0.8438\n",
      "Epoch 99/1000\n",
      "\u001B[1m31/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6136 - loss: 0.9986 \n",
      "Epoch 99: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.6150 - loss: 0.9954 - val_accuracy: 0.6080 - val_loss: 0.8493\n",
      "Epoch 100/1000\n",
      "\u001B[1m31/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6085 - loss: 0.9801 \n",
      "Epoch 100: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.6076 - loss: 0.9811 - val_accuracy: 0.6215 - val_loss: 0.8353\n",
      "Epoch 101/1000\n",
      "\u001B[1m28/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6248 - loss: 0.9617 \n",
      "Epoch 101: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.6254 - loss: 0.9624 - val_accuracy: 0.6114 - val_loss: 0.8490\n",
      "Epoch 102/1000\n",
      "\u001B[1m29/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6255 - loss: 0.9704 \n",
      "Epoch 102: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.6255 - loss: 0.9699 - val_accuracy: 0.6222 - val_loss: 0.8540\n",
      "Epoch 103/1000\n",
      "\u001B[1m28/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6325 - loss: 0.9673 \n",
      "Epoch 103: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6292 - loss: 0.9705 - val_accuracy: 0.6107 - val_loss: 0.8508\n",
      "Epoch 104/1000\n",
      "\u001B[1m24/35\u001B[0m \u001B[32m━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6190 - loss: 0.9482 \n",
      "Epoch 104: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6181 - loss: 0.9533 - val_accuracy: 0.6114 - val_loss: 0.8522\n",
      "Epoch 105/1000\n",
      "\u001B[1m27/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6226 - loss: 0.9517 \n",
      "Epoch 105: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6233 - loss: 0.9537 - val_accuracy: 0.6026 - val_loss: 0.8604\n",
      "Epoch 106/1000\n",
      "\u001B[1m23/35\u001B[0m \u001B[32m━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6225 - loss: 0.9557 \n",
      "Epoch 106: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6212 - loss: 0.9626 - val_accuracy: 0.6148 - val_loss: 0.8536\n",
      "Epoch 107/1000\n",
      "\u001B[1m26/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6235 - loss: 0.9568 \n",
      "Epoch 107: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6233 - loss: 0.9576 - val_accuracy: 0.5897 - val_loss: 0.8556\n",
      "Epoch 108/1000\n",
      "\u001B[1m27/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6230 - loss: 0.9624 \n",
      "Epoch 108: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6220 - loss: 0.9607 - val_accuracy: 0.6060 - val_loss: 0.8486\n",
      "Epoch 109/1000\n",
      "\u001B[1m28/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6254 - loss: 0.9760 \n",
      "Epoch 109: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6251 - loss: 0.9751 - val_accuracy: 0.6080 - val_loss: 0.8403\n",
      "Epoch 110/1000\n",
      "\u001B[1m23/35\u001B[0m \u001B[32m━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6372 - loss: 0.9521 \n",
      "Epoch 110: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.6391 - loss: 0.9522 - val_accuracy: 0.6012 - val_loss: 0.8456\n",
      "Epoch 111/1000\n",
      "\u001B[1m21/35\u001B[0m \u001B[32m━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6591 - loss: 0.9291 \n",
      "Epoch 111: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6545 - loss: 0.9333 - val_accuracy: 0.6053 - val_loss: 0.8492\n",
      "Epoch 112/1000\n",
      "\u001B[1m28/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6327 - loss: 0.9519 \n",
      "Epoch 112: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.6304 - loss: 0.9559 - val_accuracy: 0.6127 - val_loss: 0.8327\n",
      "Epoch 113/1000\n",
      "\u001B[1m26/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6314 - loss: 0.9589 \n",
      "Epoch 113: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6285 - loss: 0.9617 - val_accuracy: 0.6073 - val_loss: 0.8327\n",
      "Epoch 114/1000\n",
      "\u001B[1m28/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6491 - loss: 0.9399 \n",
      "Epoch 114: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6467 - loss: 0.9420 - val_accuracy: 0.5972 - val_loss: 0.8528\n",
      "Epoch 115/1000\n",
      "\u001B[1m28/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6111 - loss: 0.9521 \n",
      "Epoch 115: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.6133 - loss: 0.9506 - val_accuracy: 0.6209 - val_loss: 0.8348\n",
      "Epoch 116/1000\n",
      "\u001B[1m27/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6363 - loss: 0.9357 \n",
      "Epoch 116: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6340 - loss: 0.9399 - val_accuracy: 0.6141 - val_loss: 0.8258\n",
      "Epoch 117/1000\n",
      "\u001B[1m14/35\u001B[0m \u001B[32m━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.6046 - loss: 0.9756 \n",
      "Epoch 117: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6167 - loss: 0.9681 - val_accuracy: 0.6134 - val_loss: 0.8212\n",
      "Epoch 118/1000\n",
      "\u001B[1m24/35\u001B[0m \u001B[32m━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6304 - loss: 0.9634 \n",
      "Epoch 118: saving model to model/keypoint_classifier/keypoint_classifier.keras\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6319 - loss: 0.9596 - val_accuracy: 0.5985 - val_loss: 0.8381\n",
      "Epoch 119/1000\n",
      "\u001B[1m25/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6202 - loss: 0.9419 \n",
      "Epoch 119: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.6213 - loss: 0.9456 - val_accuracy: 0.6073 - val_loss: 0.8324\n",
      "Epoch 120/1000\n",
      "\u001B[1m25/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6208 - loss: 0.9592 \n",
      "Epoch 120: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.6219 - loss: 0.9586 - val_accuracy: 0.6121 - val_loss: 0.8318\n",
      "Epoch 121/1000\n",
      "\u001B[1m26/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6463 - loss: 0.9306 \n",
      "Epoch 121: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.6456 - loss: 0.9328 - val_accuracy: 0.5917 - val_loss: 0.8435\n",
      "Epoch 122/1000\n",
      "\u001B[1m26/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6245 - loss: 0.9630 \n",
      "Epoch 122: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6267 - loss: 0.9601 - val_accuracy: 0.5897 - val_loss: 0.8410\n",
      "Epoch 123/1000\n",
      "\u001B[1m28/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.6304 - loss: 0.9406\n",
      "Epoch 123: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.6288 - loss: 0.9442 - val_accuracy: 0.6066 - val_loss: 0.8310\n",
      "Epoch 124/1000\n",
      "\u001B[1m32/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6312 - loss: 0.9429\n",
      "Epoch 124: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.6310 - loss: 0.9434 - val_accuracy: 0.6046 - val_loss: 0.8394\n",
      "Epoch 125/1000\n",
      "\u001B[1m34/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6330 - loss: 0.9443\n",
      "Epoch 125: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.6328 - loss: 0.9447 - val_accuracy: 0.6195 - val_loss: 0.8207\n",
      "Epoch 126/1000\n",
      "\u001B[1m25/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6309 - loss: 0.9727 \n",
      "Epoch 126: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6308 - loss: 0.9657 - val_accuracy: 0.6154 - val_loss: 0.8235\n",
      "Epoch 127/1000\n",
      "\u001B[1m27/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6331 - loss: 0.9463 \n",
      "Epoch 127: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.6327 - loss: 0.9479 - val_accuracy: 0.6175 - val_loss: 0.8132\n",
      "Epoch 128/1000\n",
      "\u001B[1m32/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6381 - loss: 0.9339\n",
      "Epoch 128: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.6380 - loss: 0.9342 - val_accuracy: 0.6121 - val_loss: 0.8241\n",
      "Epoch 129/1000\n",
      "\u001B[1m19/35\u001B[0m \u001B[32m━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6290 - loss: 0.9488 \n",
      "Epoch 129: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.6302 - loss: 0.9489 - val_accuracy: 0.6175 - val_loss: 0.8218\n",
      "Epoch 130/1000\n",
      "\u001B[1m24/35\u001B[0m \u001B[32m━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6558 - loss: 0.9393 \n",
      "Epoch 130: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6528 - loss: 0.9377 - val_accuracy: 0.6249 - val_loss: 0.8049\n",
      "Epoch 131/1000\n",
      "\u001B[1m22/35\u001B[0m \u001B[32m━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6257 - loss: 0.9475 \n",
      "Epoch 131: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.6304 - loss: 0.9392 - val_accuracy: 0.6195 - val_loss: 0.8106\n",
      "Epoch 132/1000\n",
      "\u001B[1m23/35\u001B[0m \u001B[32m━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6614 - loss: 0.9213 \n",
      "Epoch 132: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6559 - loss: 0.9254 - val_accuracy: 0.6114 - val_loss: 0.8233\n",
      "Epoch 133/1000\n",
      "\u001B[1m29/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6448 - loss: 0.9152 \n",
      "Epoch 133: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.6421 - loss: 0.9205 - val_accuracy: 0.6195 - val_loss: 0.8124\n",
      "Epoch 134/1000\n",
      "\u001B[1m22/35\u001B[0m \u001B[32m━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6410 - loss: 0.9420 \n",
      "Epoch 134: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6403 - loss: 0.9406 - val_accuracy: 0.6019 - val_loss: 0.8213\n",
      "Epoch 135/1000\n",
      "\u001B[1m21/35\u001B[0m \u001B[32m━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6504 - loss: 0.9148 \n",
      "Epoch 135: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.6501 - loss: 0.9163 - val_accuracy: 0.6148 - val_loss: 0.8193\n",
      "Epoch 136/1000\n",
      "\u001B[1m22/35\u001B[0m \u001B[32m━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6558 - loss: 0.9034 \n",
      "Epoch 136: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.6506 - loss: 0.9127 - val_accuracy: 0.6209 - val_loss: 0.8059\n",
      "Epoch 137/1000\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6491 - loss: 0.9143\n",
      "Epoch 137: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.6491 - loss: 0.9143 - val_accuracy: 0.6100 - val_loss: 0.8208\n",
      "Epoch 138/1000\n",
      "\u001B[1m29/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6393 - loss: 0.9342 \n",
      "Epoch 138: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.6390 - loss: 0.9352 - val_accuracy: 0.6175 - val_loss: 0.8096\n",
      "Epoch 139/1000\n",
      "\u001B[1m18/35\u001B[0m \u001B[32m━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6249 - loss: 0.9385 \n",
      "Epoch 139: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.6306 - loss: 0.9264 - val_accuracy: 0.6148 - val_loss: 0.8051\n",
      "Epoch 140/1000\n",
      "\u001B[1m22/35\u001B[0m \u001B[32m━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6391 - loss: 0.9535 \n",
      "Epoch 140: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.6419 - loss: 0.9454 - val_accuracy: 0.5951 - val_loss: 0.8242\n",
      "Epoch 141/1000\n",
      "\u001B[1m33/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6417 - loss: 0.9336\n",
      "Epoch 141: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.6417 - loss: 0.9333 - val_accuracy: 0.6053 - val_loss: 0.8148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/1000\n",
      "\u001B[1m34/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6477 - loss: 0.9081\n",
      "Epoch 142: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.6470 - loss: 0.9095 - val_accuracy: 0.6209 - val_loss: 0.8028\n",
      "Epoch 143/1000\n",
      "\u001B[1m33/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6328 - loss: 0.9741\n",
      "Epoch 143: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.6334 - loss: 0.9708 - val_accuracy: 0.6181 - val_loss: 0.8052\n",
      "Epoch 144/1000\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6367 - loss: 0.9087\n",
      "Epoch 144: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.6366 - loss: 0.9093 - val_accuracy: 0.6175 - val_loss: 0.8062\n",
      "Epoch 145/1000\n",
      "\u001B[1m20/35\u001B[0m \u001B[32m━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6652 - loss: 0.9084 \n",
      "Epoch 145: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.6590 - loss: 0.9154 - val_accuracy: 0.6087 - val_loss: 0.8059\n",
      "Epoch 146/1000\n",
      "\u001B[1m32/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6364 - loss: 0.9450\n",
      "Epoch 146: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.6373 - loss: 0.9444 - val_accuracy: 0.6127 - val_loss: 0.8105\n",
      "Epoch 147/1000\n",
      "\u001B[1m14/35\u001B[0m \u001B[32m━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.6366 - loss: 0.9428 \n",
      "Epoch 147: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.6404 - loss: 0.9272 - val_accuracy: 0.6154 - val_loss: 0.8061\n",
      "Epoch 148/1000\n",
      "\u001B[1m25/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6444 - loss: 0.9254 \n",
      "Epoch 148: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6466 - loss: 0.9216 - val_accuracy: 0.6060 - val_loss: 0.8132\n",
      "Epoch 149/1000\n",
      "\u001B[1m27/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6394 - loss: 0.9191 \n",
      "Epoch 149: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6404 - loss: 0.9213 - val_accuracy: 0.6154 - val_loss: 0.8032\n",
      "Epoch 150/1000\n",
      "\u001B[1m29/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6481 - loss: 0.8956 \n",
      "Epoch 150: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6460 - loss: 0.9005 - val_accuracy: 0.6181 - val_loss: 0.8054\n",
      "Epoch 151/1000\n",
      "\u001B[1m22/35\u001B[0m \u001B[32m━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6568 - loss: 0.8982 \n",
      "Epoch 151: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6563 - loss: 0.8988 - val_accuracy: 0.6121 - val_loss: 0.8037\n",
      "Epoch 152/1000\n",
      "\u001B[1m27/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6472 - loss: 0.9006 \n",
      "Epoch 152: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6463 - loss: 0.9056 - val_accuracy: 0.6222 - val_loss: 0.7961\n",
      "Epoch 153/1000\n",
      "\u001B[1m29/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6598 - loss: 0.8945 \n",
      "Epoch 153: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6598 - loss: 0.8962 - val_accuracy: 0.6168 - val_loss: 0.7966\n",
      "Epoch 154/1000\n",
      "\u001B[1m22/35\u001B[0m \u001B[32m━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6558 - loss: 0.9531 \n",
      "Epoch 154: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6508 - loss: 0.9451 - val_accuracy: 0.6093 - val_loss: 0.8186\n",
      "Epoch 155/1000\n",
      "\u001B[1m28/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6407 - loss: 0.9178 \n",
      "Epoch 155: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.6412 - loss: 0.9194 - val_accuracy: 0.6229 - val_loss: 0.7892\n",
      "Epoch 156/1000\n",
      "\u001B[1m23/35\u001B[0m \u001B[32m━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6527 - loss: 0.9135 \n",
      "Epoch 156: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.6491 - loss: 0.9166 - val_accuracy: 0.6154 - val_loss: 0.7894\n",
      "Epoch 157/1000\n",
      "\u001B[1m20/35\u001B[0m \u001B[32m━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6518 - loss: 0.8965 \n",
      "Epoch 157: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - accuracy: 0.6514 - loss: 0.9021 - val_accuracy: 0.6114 - val_loss: 0.7903\n",
      "Epoch 158/1000\n",
      "\u001B[1m21/35\u001B[0m \u001B[32m━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6278 - loss: 0.9430 \n",
      "Epoch 158: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.6352 - loss: 0.9357 - val_accuracy: 0.6175 - val_loss: 0.7944\n",
      "Epoch 159/1000\n",
      "\u001B[1m24/35\u001B[0m \u001B[32m━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6536 - loss: 0.9202 \n",
      "Epoch 159: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6541 - loss: 0.9166 - val_accuracy: 0.6222 - val_loss: 0.7861\n",
      "Epoch 160/1000\n",
      "\u001B[1m27/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6353 - loss: 0.8979 \n",
      "Epoch 160: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.6381 - loss: 0.8972 - val_accuracy: 0.6141 - val_loss: 0.7969\n",
      "Epoch 161/1000\n",
      "\u001B[1m22/35\u001B[0m \u001B[32m━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6209 - loss: 0.9526 \n",
      "Epoch 161: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.6283 - loss: 0.9435 - val_accuracy: 0.6026 - val_loss: 0.8088\n",
      "Epoch 162/1000\n",
      "\u001B[1m23/35\u001B[0m \u001B[32m━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6695 - loss: 0.8828 \n",
      "Epoch 162: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.6653 - loss: 0.8883 - val_accuracy: 0.6249 - val_loss: 0.7862\n",
      "Epoch 163/1000\n",
      "\u001B[1m20/35\u001B[0m \u001B[32m━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6529 - loss: 0.9148 \n",
      "Epoch 163: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.6564 - loss: 0.9087 - val_accuracy: 0.6154 - val_loss: 0.7871\n",
      "Epoch 164/1000\n",
      "\u001B[1m20/35\u001B[0m \u001B[32m━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6703 - loss: 0.8847 \n",
      "Epoch 164: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.6644 - loss: 0.8927 - val_accuracy: 0.6053 - val_loss: 0.8049\n",
      "Epoch 165/1000\n",
      "\u001B[1m32/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6426 - loss: 0.9279\n",
      "Epoch 165: saving model to model/keypoint_classifier/keypoint_classifier.keras\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.6433 - loss: 0.9264 - val_accuracy: 0.6019 - val_loss: 0.8034\n",
      "Epoch 166/1000\n",
      "\u001B[1m22/35\u001B[0m \u001B[32m━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6259 - loss: 0.9446 \n",
      "Epoch 166: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6326 - loss: 0.9377 - val_accuracy: 0.6236 - val_loss: 0.7853\n",
      "Epoch 167/1000\n",
      "\u001B[1m31/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6465 - loss: 0.9206 \n",
      "Epoch 167: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6479 - loss: 0.9188 - val_accuracy: 0.6141 - val_loss: 0.7867\n",
      "Epoch 168/1000\n",
      "\u001B[1m31/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6602 - loss: 0.9149 \n",
      "Epoch 168: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.6597 - loss: 0.9140 - val_accuracy: 0.6276 - val_loss: 0.7749\n",
      "Epoch 169/1000\n",
      "\u001B[1m27/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6403 - loss: 0.9118 \n",
      "Epoch 169: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6428 - loss: 0.9105 - val_accuracy: 0.6181 - val_loss: 0.7878\n",
      "Epoch 170/1000\n",
      "\u001B[1m25/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6256 - loss: 0.9619 \n",
      "Epoch 170: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6324 - loss: 0.9471 - val_accuracy: 0.6107 - val_loss: 0.7974\n",
      "Epoch 171/1000\n",
      "\u001B[1m26/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6565 - loss: 0.9076 \n",
      "Epoch 171: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6557 - loss: 0.9070 - val_accuracy: 0.6242 - val_loss: 0.7761\n",
      "Epoch 172/1000\n",
      "\u001B[1m30/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6688 - loss: 0.8920 \n",
      "Epoch 172: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.6681 - loss: 0.8931 - val_accuracy: 0.6236 - val_loss: 0.7789\n",
      "Epoch 173/1000\n",
      "\u001B[1m29/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6535 - loss: 0.9116 \n",
      "Epoch 173: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.6526 - loss: 0.9117 - val_accuracy: 0.6209 - val_loss: 0.7752\n",
      "Epoch 174/1000\n",
      "\u001B[1m28/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6535 - loss: 0.9026 \n",
      "Epoch 174: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.6539 - loss: 0.9005 - val_accuracy: 0.6364 - val_loss: 0.7560\n",
      "Epoch 175/1000\n",
      "\u001B[1m26/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6446 - loss: 0.8934 \n",
      "Epoch 175: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6469 - loss: 0.8931 - val_accuracy: 0.6330 - val_loss: 0.7652\n",
      "Epoch 176/1000\n",
      "\u001B[1m21/35\u001B[0m \u001B[32m━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6703 - loss: 0.8477 \n",
      "Epoch 176: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6622 - loss: 0.8618 - val_accuracy: 0.6290 - val_loss: 0.7666\n",
      "Epoch 177/1000\n",
      "\u001B[1m24/35\u001B[0m \u001B[32m━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6486 - loss: 0.9065 \n",
      "Epoch 177: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6509 - loss: 0.9039 - val_accuracy: 0.6202 - val_loss: 0.7788\n",
      "Epoch 178/1000\n",
      "\u001B[1m24/35\u001B[0m \u001B[32m━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6473 - loss: 0.9101 \n",
      "Epoch 178: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6502 - loss: 0.9049 - val_accuracy: 0.6256 - val_loss: 0.7716\n",
      "Epoch 179/1000\n",
      "\u001B[1m28/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6610 - loss: 0.9069 \n",
      "Epoch 179: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.6612 - loss: 0.9043 - val_accuracy: 0.6317 - val_loss: 0.7645\n",
      "Epoch 180/1000\n",
      "\u001B[1m22/35\u001B[0m \u001B[32m━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6469 - loss: 0.9095 \n",
      "Epoch 180: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6485 - loss: 0.9107 - val_accuracy: 0.6188 - val_loss: 0.7841\n",
      "Epoch 181/1000\n",
      "\u001B[1m24/35\u001B[0m \u001B[32m━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6809 - loss: 0.8679 \n",
      "Epoch 181: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6778 - loss: 0.8715 - val_accuracy: 0.6222 - val_loss: 0.7714\n",
      "Epoch 182/1000\n",
      "\u001B[1m25/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6686 - loss: 0.8685 \n",
      "Epoch 182: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.6669 - loss: 0.8733 - val_accuracy: 0.6276 - val_loss: 0.7663\n",
      "Epoch 183/1000\n",
      "\u001B[1m25/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6703 - loss: 0.8920 \n",
      "Epoch 183: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6676 - loss: 0.8914 - val_accuracy: 0.6134 - val_loss: 0.7841\n",
      "Epoch 184/1000\n",
      "\u001B[1m33/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6709 - loss: 0.8719 \n",
      "Epoch 184: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6707 - loss: 0.8724 - val_accuracy: 0.6121 - val_loss: 0.7882\n",
      "Epoch 185/1000\n",
      "\u001B[1m22/35\u001B[0m \u001B[32m━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6716 - loss: 0.8827 \n",
      "Epoch 185: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6707 - loss: 0.8886 - val_accuracy: 0.5972 - val_loss: 0.7987\n",
      "Epoch 186/1000\n",
      "\u001B[1m25/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6664 - loss: 0.8874 \n",
      "Epoch 186: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6626 - loss: 0.8935 - val_accuracy: 0.6188 - val_loss: 0.7782\n",
      "Epoch 187/1000\n",
      "\u001B[1m18/35\u001B[0m \u001B[32m━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6651 - loss: 0.9164 \n",
      "Epoch 187: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.6655 - loss: 0.9021 - val_accuracy: 0.6161 - val_loss: 0.7750\n",
      "Epoch 188/1000\n",
      "\u001B[1m19/35\u001B[0m \u001B[32m━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6636 - loss: 0.8931 \n",
      "Epoch 188: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.6602 - loss: 0.8978 - val_accuracy: 0.6229 - val_loss: 0.7744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 189/1000\n",
      "\u001B[1m28/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6575 - loss: 0.9073 \n",
      "Epoch 189: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.6571 - loss: 0.9044 - val_accuracy: 0.6263 - val_loss: 0.7617\n",
      "Epoch 190/1000\n",
      "\u001B[1m30/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6503 - loss: 0.9042 \n",
      "Epoch 190: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6522 - loss: 0.9009 - val_accuracy: 0.6202 - val_loss: 0.7775\n",
      "Epoch 191/1000\n",
      "\u001B[1m30/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6594 - loss: 0.8793 \n",
      "Epoch 191: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.6589 - loss: 0.8818 - val_accuracy: 0.6297 - val_loss: 0.7703\n",
      "Epoch 192/1000\n",
      "\u001B[1m23/35\u001B[0m \u001B[32m━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6597 - loss: 0.8984 \n",
      "Epoch 192: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.6605 - loss: 0.8928 - val_accuracy: 0.6283 - val_loss: 0.7653\n",
      "Epoch 193/1000\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6612 - loss: 0.8866 \n",
      "Epoch 193: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.6611 - loss: 0.8868 - val_accuracy: 0.6337 - val_loss: 0.7565\n",
      "Epoch 194/1000\n",
      "\u001B[1m28/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6645 - loss: 0.9074 \n",
      "Epoch 194: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
      "\u001B[1m35/35\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.6657 - loss: 0.9017 - val_accuracy: 0.6269 - val_loss: 0.7712\n",
      "Epoch 194: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x222d834a290>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=1000,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[cp_callback, es_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m12/12\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6308 - loss: 0.7585 \n"
     ]
    }
   ],
   "source": [
    "# モデル評価\n",
    "val_loss, val_acc = model.evaluate(X_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存したモデルのロード\n",
    "model = tf.keras.models.load_model(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 99ms/step\n",
      "[0.11212792 0.6407486  0.21171628 0.00662802 0.02294772 0.00479771\n",
      " 0.0010338 ]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# 推論テスト\n",
    "predict_result = model.predict(np.array([X_test[0]]))\n",
    "print(np.squeeze(predict_result))\n",
    "print(np.argmax(np.squeeze(predict_result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 混同行列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\project_her\\hand-gesture-recognition-mediapipe\\hernew\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\project_her\\hand-gesture-recognition-mediapipe\\hernew\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\project_her\\hand-gesture-recognition-mediapipe\\hernew\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\project_her\\hand-gesture-recognition-mediapipe\\hernew\\lib\\site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\project_her\\hand-gesture-recognition-mediapipe\\hernew\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\project_her\\hand-gesture-recognition-mediapipe\\hernew\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\project_her\\hand-gesture-recognition-mediapipe\\hernew\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\project_her\\hand-gesture-recognition-mediapipe\\hernew\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\project_her\\hand-gesture-recognition-mediapipe\\hernew\\lib\\site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\project_her\\hand-gesture-recognition-mediapipe\\hernew\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\project_her\\hand-gesture-recognition-mediapipe\\hernew\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.2\n",
      "[notice] To update, run: C:\\Project_HER\\hand-gesture-recognition-mediapipe\\hernew\\scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\project_her\\hand-gesture-recognition-mediapipe\\hernew\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\project_her\\hand-gesture-recognition-mediapipe\\hernew\\lib\\site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\project_her\\hand-gesture-recognition-mediapipe\\hernew\\lib\\site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\project_her\\hand-gesture-recognition-mediapipe\\hernew\\lib\\site-packages (from seaborn) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\project_her\\hand-gesture-recognition-mediapipe\\hernew\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\project_her\\hand-gesture-recognition-mediapipe\\hernew\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\project_her\\hand-gesture-recognition-mediapipe\\hernew\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\project_her\\hand-gesture-recognition-mediapipe\\hernew\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\project_her\\hand-gesture-recognition-mediapipe\\hernew\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\project_her\\hand-gesture-recognition-mediapipe\\hernew\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\project_her\\hand-gesture-recognition-mediapipe\\hernew\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\project_her\\hand-gesture-recognition-mediapipe\\hernew\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\project_her\\hand-gesture-recognition-mediapipe\\hernew\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\project_her\\hand-gesture-recognition-mediapipe\\hernew\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\project_her\\hand-gesture-recognition-mediapipe\\hernew\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.2\n",
      "[notice] To update, run: C:\\Project_HER\\hand-gesture-recognition-mediapipe\\hernew\\scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\project_her\\hand-gesture-recognition-mediapipe\\hernew\\lib\\site-packages (2.2.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\project_her\\hand-gesture-recognition-mediapipe\\hernew\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\project_her\\hand-gesture-recognition-mediapipe\\hernew\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\project_her\\hand-gesture-recognition-mediapipe\\hernew\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\project_her\\hand-gesture-recognition-mediapipe\\hernew\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\project_her\\hand-gesture-recognition-mediapipe\\hernew\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.2\n",
      "[notice] To update, run: C:\\Project_HER\\hand-gesture-recognition-mediapipe\\hernew\\scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m47/47\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.89      0.92       404\n",
      "           1       0.67      0.16      0.26       378\n",
      "           2       0.50      1.00      0.67       330\n",
      "           3       0.00      0.00      0.00        77\n",
      "           4       0.50      0.89      0.64       198\n",
      "           5       0.00      0.00      0.00        90\n",
      "\n",
      "    accuracy                           0.63      1477\n",
      "   macro avg       0.44      0.49      0.41      1477\n",
      "weighted avg       0.61      0.63      0.55      1477\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Project_HER\\hand-gesture-recognition-mediapipe\\hernew\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Project_HER\\hand-gesture-recognition-mediapipe\\hernew\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Project_HER\\hand-gesture-recognition-mediapipe\\hernew\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def print_confusion_matrix(y_true, y_pred, report=True):\n",
    "    labels = sorted(list(set(y_true)))\n",
    "    cmx_data = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    \n",
    "    df_cmx = pd.DataFrame(cmx_data, index=labels, columns=labels)\n",
    " \n",
    "    fig, ax = plt.subplots(figsize=(7, 6))\n",
    "    sns.heatmap(df_cmx, annot=True, fmt='g' ,square=False)\n",
    "    ax.set_ylim(len(set(y_true)), 0)\n",
    "    plt.show()\n",
    "    \n",
    "    if report:\n",
    "        print('Classification Report')\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "Y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "print_confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow-Lite用のモデルへ変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論専用のモデルとして保存\n",
    "model.save(model_save_path, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\sdswa\\AppData\\Local\\Temp\\tmp7s78aewf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\sdswa\\AppData\\Local\\Temp\\tmp7s78aewf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\sdswa\\AppData\\Local\\Temp\\tmp7s78aewf'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 42), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 7), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2348678657424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2348678656272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2348678650128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2348678885648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2348678884112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2348678883920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2348678884496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2348678883728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7316"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデルを変換(量子化)\n",
    "tflite_save_path = 'model/keypoint_classifier/keypoint_classifier.tflite'\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quantized_model = converter.convert()\n",
    "\n",
    "open(tflite_save_path, 'wb').write(tflite_quantized_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 推論テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=tflite_save_path)\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入出力テンソルを取得\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.set_tensor(input_details[0]['index'], np.array([X_test[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 3.01 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 推論実施\n",
    "interpreter.invoke()\n",
    "tflite_results = interpreter.get_tensor(output_details[0]['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1121279  0.6407486  0.21171626 0.00662802 0.02294772 0.00479771\n",
      " 0.0010338 ]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(np.squeeze(tflite_results))\n",
    "print(np.argmax(np.squeeze(tflite_results)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kernel3",
   "language": "python",
   "name": "kernel3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
